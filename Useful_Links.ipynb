{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Useful Links.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMO+FwYl7kfTrcKG/kSLBkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riszwinger/tensorflow_cert/blob/main/Useful_Links.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkymOKTsWsIp"
      },
      "source": [
        "### [Epoch vs Batch Size vs Iteration](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
        "\n",
        "\n",
        "\n",
        "We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n",
        "\n",
        "\n",
        "## [steps_per_epoch](https://datascience.stackexchange.com/questions/47405/what-to-set-in-steps-per-epoch-in-keras-fit-generator)\n",
        "\n",
        "steps_per_epoch=Number of Iteration=num_of_rows_train_data // batch_size\n",
        "\n",
        "You can set it equal to num_samples // batch_size, which is a typical choice.\n",
        "\n",
        "However, steps_per_epoch give you the chance to \"trick\" the generator when updating the learning rate using ReduceLROnPlateau() callback, because this callback checks the drop of the loss once each epoch has finished. If the loss has stagnated for a patience number of consecutive epochs, the callback decreases the learning rate to \"slow-cook\" the network. If your dataset is huge, as it is usually the case when you need to use generators, you would probably like to decay the learning rate within a single epoch (since it includes a big number of data). This can be achieved by setting steps_per_epoch to a value that is less than num_samples // batch_size without affecting the overall number of training epochs of your model.\n",
        "\n",
        "**validation_steps** similar to steps_per_epoch but on the validation data set instead on the training data. \n",
        "\n",
        "[steps_per_epoch deep dive](https://github.com/keras-team/keras/issues/10164)\n",
        "*The steps_per_epoch argument is not related to how the samples are fed. It's just a number that is used to define an 'epoch'. Some people want their callbacks to be called more often than once per \"real epoch\"*. So they can set a lower steps_per_epoch.\n",
        "\n",
        "The unseen samples will be seen in the second epoch\n",
        "Your model will see the same samples multiple times.\n",
        "*real epoch is a loop over all of your data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvJS-okOWw8M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}