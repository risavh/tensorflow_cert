{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Useful_Links.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1ciBWFMxuUA4QLAq2dtDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riszwinger/tensorflow_cert/blob/main/Useful_Links.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkymOKTsWsIp"
      },
      "source": [
        "### [Epoch vs Batch Size vs Iteration](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
        "\n",
        "\n",
        "\n",
        "We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.\n",
        "\n",
        "\n",
        "## [steps_per_epoch](https://datascience.stackexchange.com/questions/47405/what-to-set-in-steps-per-epoch-in-keras-fit-generator)\n",
        "\n",
        "steps_per_epoch=Number of Iteration=num_of_rows_train_data // batch_size\n",
        "\n",
        "You can set it equal to num_samples // batch_size, which is a typical choice.\n",
        "\n",
        "However, steps_per_epoch give you the chance to \"trick\" the generator when updating the learning rate using ReduceLROnPlateau() callback, because this callback checks the drop of the loss once each epoch has finished. If the loss has stagnated for a patience number of consecutive epochs, the callback decreases the learning rate to \"slow-cook\" the network. If your dataset is huge, as it is usually the case when you need to use generators, you would probably like to decay the learning rate within a single epoch (since it includes a big number of data). This can be achieved by setting steps_per_epoch to a value that is less than num_samples // batch_size without affecting the overall number of training epochs of your model.\n",
        "\n",
        "**validation_steps** similar to steps_per_epoch but on the validation data set instead on the training data. \n",
        "\n",
        "[steps_per_epoch deep dive](https://github.com/keras-team/keras/issues/10164)\n",
        "*The steps_per_epoch argument is not related to how the samples are fed. It's just a number that is used to define an 'epoch'. Some people want their callbacks to be called more often than once per \"real epoch\"*. So they can set a lower steps_per_epoch.\n",
        "\n",
        "The unseen samples will be seen in the second epoch\n",
        "Your model will see the same samples multiple times.\n",
        "*real epoch is a loop over all of your data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXbzSMyKyA9b"
      },
      "source": [
        "## NLP\n",
        "\n",
        "- [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) after TextVectorization sets up the vocab which run only on the text data.\n",
        "- The shape of input data needs to be rank-1 , shape=(1,) for the TextVectorization Layer.\n",
        "- There is a performance difference to keep in mind when choosing where to apply your TextVectorization layer. Using it outside of your model enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So, if you're training your model on the GPU, you probably want to go with this option to get the best performance while developing your model, then switch to including the TextVectorization layer inside your model when you're ready to prepare for deployment.\n",
        "- **Don't use keywords like \"train\" in code.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUHi0y8ikpGm"
      },
      "source": [
        "## Time Series\n",
        "\n",
        "- [Basic](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)\n",
        "- [Shape](https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e)\n",
        "- [RNN Shape](https://stackoverflow.com/questions/47268608/confusion-about-keras-rnn-input-shape-requirement)\n",
        "- [CNN Shape](https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ep3IvHTraZ"
      },
      "source": [
        "## Errors\n",
        "\n",
        "\n",
        "### <u> Issue-1</u>\n",
        "```history=model.fit(train_datagen,epochs=10,steps_per_epoch=STEP_PER_EPOCHS,validation_data=val_datagen,validation_steps=VALIDATION_SETPS)```\n",
        "\n",
        "\n",
        "**InvalidArgumentError**:  logits and labels must have the same first dimension, got logits shape [32,3] and labels shape [96]\n",
        "\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-61-78e7226bd14d>:3) ]] [Op:__inference_train_function_1056]\n",
        "\n",
        "\n",
        "**Solution:**\n",
        "\n",
        "1. Looks like using wrong loss fnx in compile (SparseCategrocialCrossEntropy(from_logits=True) , may be changing this would help.\n",
        "2. Checked the input data.\n",
        "```\n",
        "img,lbl=next(iter(train_datagen))\n",
        "img[0].shape ##(180, 180, 3)\n",
        "lbl[0] ##array([1., 0., 0.], dtype=float32)\n",
        "```\n",
        "3. Change the loss to CategoricalCrossentropy as below.\n",
        "```\n",
        "model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "*Takeaway* : \n",
        "- sparse_categorical_crossentropy expects integer labels, as it does the one-hot encoding itself (hence, sparse).\n",
        "\n",
        "- CategoricalCrossentropy : labels to be provided in a one_hot representation.\n",
        "\n",
        "\n",
        "### <u> Issue-2</u>\n",
        "\n",
        "*WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 945 batches). You may need to use the repeat() function when building your dataset.*\n",
        "\n",
        "**Solution**\n",
        "\n",
        "- Make sure to recacluate the values for Steps_per_epoch, and check the value to confirm\n",
        "\n",
        "\n",
        "### <u> Issue-3</u>\n",
        "\n",
        "TypeError: a bytes-like object is required, not 'dict'\n",
        "\n",
        "```\n",
        "train_txt[:2]\n",
        "#[b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. ,\n",
        "# b'I have been known to fall asl\n",
        "#]\n",
        "tokenObj.fit_on_texts(train_txt)\n",
        "```\n",
        "**Solution**\n",
        "\n",
        "-  need to decode to make it like utf-8\n",
        "```\n",
        "tokenObj.fit_on_texts([i.decode('utf-8') for i in train_txt])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkjEbsl68mGC"
      },
      "source": [
        "## GoodReads\n",
        "\n",
        "- [medium link](https://towardsdatascience.com/nine-tools-i-wish-i-mastered-before-my-phd-in-machine-learning-708c6dcb2fb0)\n",
        "\n",
        "## Must reads\n",
        "\n",
        "- [Google Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alWGFPTczS3E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}